from typing import Any, Dict

from langchain_core.messages import ToolMessage

from core.state import AgentState
from logger import logger


async def tool_execute_node(state: AgentState) -> Dict[str, Any]:
    """
    Executes tool calls generated by the LLM natively.

    This node iterates through 'tool_calls' in the last message,
    executes the corresponding function from 'tools_dict',
    and returns ToolMessages that LangGraph will merge into the state.
    """
    last_message = state["messages"][-1]
    tool_calls = getattr(last_message, "tool_calls", [])

    logger.info(f"Tool Execution: Processing {len(tool_calls)} tool calls.")
    tool_results = []
    execution_logs = []

    for tool_call in tool_calls:
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]
        tool_id = tool_call["id"]

        logger.info(f"Executing tool: {tool_name} with args: {tool_args}")

        try:
            tool_func = state["tools_dict"].get(tool_name)

            if tool_func:
                result = await tool_func.ainvoke(tool_args)
                result_content = str(result)
            else:
                result_content = f"Error: Tool '{tool_name}' not found in tools_dict."
                logger.error(result_content)

        except Exception as e:
            result_content = f"Execution error: {str(e)}"
            logger.error(f"Error executing {tool_name}: {e}")

        tool_results.append(
            ToolMessage(tool_call_id=tool_id, name=tool_name, content=result_content)
        )
        execution_logs.append(f"Tool '{tool_name}' executed. Status: Success")

    return {
        "messages": tool_results,
        "history": execution_logs,
        "current_agent": "tool_executor",
    }
